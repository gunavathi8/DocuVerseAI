{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18e9bfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f798a3fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a15a049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3413be0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80e77a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Artificial Intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think and learn like humans. The term can also be applied to any machine that exhibits traits associated with a human mind such as learning and problem-solving.\\n\\nAI technology is based on the principle of creating algorithms that enable computers to perform tasks that would normally require human intelligence, such as:\\n\\n1. **Learning**: AI systems can learn from data, identify patterns, and make decisions based on that data.\\n2. **Reasoning**: AI systems can draw conclusions and make decisions based on the data they have learned.\\n3. **Problem-solving**: AI systems can analyze complex problems and find solutions.\\n4. **Perception**: AI systems can interpret and understand data from sensors, such as images, speech, and text.\\n\\nThere are several types of AI, including:\\n\\n1. **Narrow or Weak AI**: Designed to perform a specific task, such as facial recognition or language translation.\\n2. **General or Strong AI**: A hypothetical AI system that possesses the ability to understand, learn, and apply knowledge across a wide range of tasks, similar to human intelligence.\\n3. **Superintelligence**: A hypothetical AI system that significantly surpasses human intelligence in all domains.\\n\\nAI is being used in various applications, including:\\n\\n1. **Virtual assistants**: Siri, Google Assistant, and Alexa are all examples of AI-powered virtual assistants.\\n2. **Image recognition**: AI is used in image recognition applications, such as facial recognition and object detection.\\n3. **Natural language processing**: AI is used in language translation and text analysis applications.\\n4. **Robotics**: AI is used in robotics to enable robots to learn and adapt to new situations.\\n5. **Healthcare**: AI is used in healthcare to analyze medical images and diagnose diseases.\\n\\nOverall, AI has the potential to revolutionize many industries and aspects of our lives, and its applications are constantly expanding.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 388, 'prompt_tokens': 39, 'total_tokens': 427, 'completion_time': 0.517333333, 'prompt_time': 0.005407774, 'queue_time': 0.049981290000000005, 'total_time': 0.522741107}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_c523237e5d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--da6a8de4-6c38-4f1d-b9e2-fb59bcbc4db0-0', usage_metadata={'input_tokens': 39, 'output_tokens': 388, 'total_tokens': 427})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fdd489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dab6401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"GOOGLE_API_KEY\"):\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b2bc0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc910337",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector=embedding_model.embed_query(\"What is AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a855a960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.03347315266728401,\n",
       " -0.03764979913830757,\n",
       " -0.050068750977516174,\n",
       " -0.008264651522040367,\n",
       " 0.04222014173865318]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c1eb90",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50577b8",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1839f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8657c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "file_path = os.path.join(os.getcwd(), \"data\", \"sample.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f18d79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(file_path)\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb870ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'g:\\\\Krish_Naik_LLMOps_Course\\\\Projects\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 0, 'page_label': '1'}, page_content='Llama 2: Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron∗ Louis Martin† Kevin Stone†\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\\nPunit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\\nYinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\\nIgor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\\nAlan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom∗\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur fine-tuned LLMs, calledLlama 2-Chat, are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nour human evaluations for helpfulness and safety, may be a suitable substitute for closed-\\nsource models. We provide a detailed description of our approach to fine-tuning and safety\\nimprovements ofLlama 2-Chatin order to enable the community to build on our work and\\ncontribute to the responsible development of LLMs.\\n∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\\n†Second author\\nContributions for all the authors can be found in Section A.1.\\narXiv:2307.09288v2  [cs.CL]  19 Jul 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'g:\\\\Krish_Naik_LLMOps_Course\\\\Projects\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 1, 'page_label': '2'}, page_content='Contents\\n1 Introduction 3\\n2 Pretraining 5\\n2.1 Pretraining Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.2 Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.3 Llama 2Pretrained Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n3 Fine-tuning 8\\n3.1 Supervised Fine-Tuning (SFT) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n3.2 Reinforcement Learning with Human Feedback (RLHF) . . . . . . . . . . . . . . . . . . . . . 9\\n3.3 System Message for Multi-Turn Consistency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n3.4 RLHF Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n4 Safety 20\\n4.1 Safety in Pretraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n4.2 Safety Fine-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n4.3 Red Teaming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\\n4.4 Safety Evaluation of Llama 2-Chat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\n5 Discussion 32\\n5.1 Learnings and Observations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n5.2 Limitations and Ethical Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\\n5.3 Responsible Release Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\n6 Related Work 35\\n7 Conclusion 36\\nA Appendix 46\\nA.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\\nA.2 Additional Details for Pretraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\\nA.3 Additional Details for Fine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\\nA.4 Additional Details for Safety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\\nA.5 Data Annotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\\nA.6 Dataset Contamination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\\nA.7 Model Card . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'g:\\\\Krish_Naik_LLMOps_Course\\\\Projects\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 2, 'page_label': '3'}, page_content='Figure 1: Helpfulness human evaluationresults forLlama\\n2-Chat compared to other open-source and closed-source\\nmodels. Human raters compared model generations on ~4k\\nprompts consisting of both single and multi-turn prompts.\\nThe95%confidenceintervalsforthisevaluationarebetween\\n1% and 2%. More details in Section 3.4.2. While reviewing\\nthese results, it is important to note that human evaluations\\ncanbenoisyduetolimitationsofthepromptset,subjectivity\\nof the review guidelines, subjectivity of individual raters,\\nand the inherent difficulty of comparing generations.\\nFigure 2: Win-rate % for helpfulness and\\nsafety between commercial-licensed base-\\nlines andLlama 2-Chat, according to GPT-\\n4. To complement the human evaluation, we\\nused a more capable model, not subject to\\nour own guidance. Green area indicates our\\nmodelisbetteraccordingtoGPT-4. Toremove\\nties, we usedwin/(win + loss). The orders in\\nwhich the model responses are presented to\\nGPT-4arerandomlyswappedtoalleviatebias.\\n1 Introduction\\nLarge Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in\\ncomplex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized\\ndomains such as programming and creative writing. They enable interaction with humans through intuitive\\nchat interfaces, which has led to rapid and widespread adoption among the general public.\\nThe capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training\\nmethodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data,\\nfollowed by alignment with human preferences via techniques such as Reinforcement Learning with Human\\nFeedback (RLHF). Although the training methodology is simple, high computational requirements have\\nlimited the development of LLMs to a few players. There have been public releases of pretrained LLMs\\n(such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that\\nmatch the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla\\n(Hoffmann et al., 2022), but none of these models are suitable substitutes for closed “product” LLMs, such\\nas ChatGPT, BARD, and Claude. These closed product LLMs are heavily fine-tuned to align with human\\npreferences, which greatly enhances their usability and safety. This step can require significant costs in\\ncompute and human annotation, and is often not transparent or easily reproducible, limiting progress within\\nthe community to advance AI alignment research.\\nIn this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs,Llama 2and\\nLlama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\nLlama 2-Chatmodels generally perform better than existing open-source models. They also appear to\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see\\nFigures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data\\nannotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally,\\nthis paper contributes a thorough description of our fine-tuning methodology and approach to improving\\nLLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and\\ncontinue to improve the safety of those models, paving the way for more responsible development of LLMs.\\nWe also share novel observations we made during the development ofLlama 2andLlama 2-Chat, such as\\nthe emergence of tool usage and temporal organization of knowledge.\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'g:\\\\Krish_Naik_LLMOps_Course\\\\Projects\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 3, 'page_label': '4'}, page_content='Figure 3: Safety human evaluation results forLlama 2-Chatcompared to other open-source and closed-\\nsource models. Human raters judged model generations for safety violations across ~2,000 adversarial\\nprompts consisting of both single and multi-turn prompts. More details can be found in Section 4.4. It is\\nimportant to caveat these safety results with the inherent bias of LLM evaluations due to limitations of the\\nprompt set, subjectivity of the review guidelines, and subjectivity of individual raters. Additionally, these\\nsafety evaluations are performed using content standards that are likely to be biased towards theLlama\\n2-Chat models.\\nWe are releasing the following models to the general public for research and commercial use‡:\\n1. Llama 2, an updated version ofLlama 1, trained on a new mix of publicly available data. We also\\nincreased the size of the pretraining corpus by 40%, doubled the context length of the model, and\\nadopted grouped-query attention (Ainslie et al., 2023). We are releasing variants ofLlama 2with\\n7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper\\nbut are not releasing.§\\n2. Llama 2-Chat, a fine-tuned version ofLlama 2that is optimized for dialogue use cases. We release\\nvariants of this model with 7B, 13B, and 70B parameters as well.\\nWe believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs,\\nLlama 2is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021;\\nSolaiman et al., 2023). Testing conducted to date has been in English and has not — and could not — cover\\nall scenarios. Therefore, before deploying any applications ofLlama 2-Chat, developers should perform\\nsafety testing and tuning tailored to their specific applications of the model. We provide a responsible use\\nguide¶ and code examples‖ to facilitate the safe deployment ofLlama 2and Llama 2-Chat. More details of\\nour responsible release strategy can be found in Section 5.3.\\nThe remainder of this paper describes our pretraining methodology (Section 2), fine-tuning methodology\\n(Section 3), approach to model safety (Section 4), key observations and insights (Section 5), relevant related\\nwork (Section 6), and conclusions (Section 7).\\n‡https://ai.meta.com/resources/models-and-libraries/llama/\\n§We are delaying the release of the 34B model due to a lack of time to sufficiently red team.\\n¶https://ai.meta.com/llama\\n‖https://github.com/facebookresearch/llama\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'g:\\\\Krish_Naik_LLMOps_Course\\\\Projects\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 4, 'page_label': '5'}, page_content='Figure 4: Training ofLlama 2-Chat: This process begins with thepretraining of Llama 2using publicly\\navailable online sources. Following this, we create an initial version ofLlama 2-Chatthrough the application\\nof supervised fine-tuning. Subsequently, the model is iteratively refined using Reinforcement Learning\\nwith Human Feedback(RLHF) methodologies, specifically through rejection sampling and Proximal Policy\\nOptimization (PPO). Throughout the RLHF stage, the accumulation ofiterative reward modeling datain\\nparallel with model enhancements is crucial to ensure the reward models remain within distribution.\\n2 Pretraining\\nTocreatethenewfamilyof Llama 2models,webeganwiththepretrainingapproachdescribedinTouvronetal.\\n(2023), using an optimized auto-regressive transformer, but made several changes to improve performance.\\nSpecifically, we performed more robust data cleaning, updated our data mixes, trained on 40% more total\\ntokens,doubledthecontextlength,andusedgrouped-queryattention(GQA)toimproveinferencescalability\\nfor our larger models. Table 1 compares the attributes of the newLlama 2models with theLlama 1models.\\n2.1 Pretraining Data\\nOur training corpus includes a new mix of data from publicly available sources, which does not include data\\nfrom Meta’s products or services. We made an effort to remove data from certain sites known to contain a\\nhigh volume of personal information about private individuals. We trained on 2 trillion tokens of data as this\\nprovides a good performance–cost trade-off, up-sampling the most factual sources in an effort to increase\\nknowledge and dampen hallucinations.\\nWe performed a variety of pretraining data investigations so that users can better understand the potential\\ncapabilities and limitations of our models; results can be found in Section 4.1.\\n2.2 Training Details\\nWe adopt most of the pretraining setting and model architecture fromLlama 1. We use the standard\\ntransformer architecture (Vaswani et al., 2017), apply pre-normalization using RMSNorm (Zhang and\\nSennrich, 2019), use the SwiGLU activation function (Shazeer, 2020), and rotary positional embeddings\\n(RoPE, Su et al. 2022). The primary architectural differences fromLlama 1include increased context length\\nand grouped-query attention (GQA). We detail in Appendix Section A.2.1 each of these differences with\\nablation experiments to demonstrate their importance.\\nHyperparameters. We trained using the AdamW optimizer (Loshchilov and Hutter, 2017), withβ1 =\\n0.9, β2 = 0.95, eps = 10−5. We use a cosine learning rate schedule, with warmup of 2000 steps, and decay\\nfinal learning rate down to 10% of the peak learning rate. We use a weight decay of0.1 and gradient clipping\\nof 1.0. Figure 5 (a) shows the training loss forLlama 2with these hyperparameters.\\n5')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f5633b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca5221cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=150,\n",
    "    length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f21d95d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af175f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "765"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d92c8fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'g:\\\\Krish_Naik_LLMOps_Course\\\\Projects\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 0, 'page_label': '1'}, page_content='Llama 2: Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron∗ Louis Martin† Kevin Stone†\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'g:\\\\Krish_Naik_LLMOps_Course\\\\Projects\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 0, 'page_label': '1'}, page_content='Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\\nPunit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\\nYinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\\nIgor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\\nAlan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'g:\\\\Krish_Naik_LLMOps_Course\\\\Projects\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 0, 'page_label': '1'}, page_content='Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom∗\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur fine-tuned LLMs, calledLlama 2-Chat, are optimized for dialogue use cases. Our'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'g:\\\\Krish_Naik_LLMOps_Course\\\\Projects\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 0, 'page_label': '1'}, page_content='Our fine-tuned LLMs, calledLlama 2-Chat, are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nour human evaluations for helpfulness and safety, may be a suitable substitute for closed-\\nsource models. We provide a detailed description of our approach to fine-tuning and safety\\nimprovements ofLlama 2-Chatin order to enable the community to build on our work and\\ncontribute to the responsible development of LLMs.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'g:\\\\Krish_Naik_LLMOps_Course\\\\Projects\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 0, 'page_label': '1'}, page_content='improvements ofLlama 2-Chatin order to enable the community to build on our work and\\ncontribute to the responsible development of LLMs.\\n∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\\n†Second author\\nContributions for all the authors can be found in Section A.1.\\narXiv:2307.09288v2  [cs.CL]  19 Jul 2023')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79feb172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'pdfTeX-1.40.25',\n",
       " 'creator': 'LaTeX with hyperref',\n",
       " 'creationdate': '2023-07-20T00:30:36+00:00',\n",
       " 'author': '',\n",
       " 'keywords': '',\n",
       " 'moddate': '2023-07-20T00:30:36+00:00',\n",
       " 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       " 'subject': '',\n",
       " 'title': '',\n",
       " 'trapped': '/False',\n",
       " 'source': 'g:\\\\Krish_Naik_LLMOps_Course\\\\Projects\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf',\n",
       " 'total_pages': 77,\n",
       " 'page': 0,\n",
       " 'page_label': '1'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[2].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a432437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c2b5972",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore=FAISS.from_documents(chunks, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59269286",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_doc = vectorstore.similarity_search(\"what are limitations and ethical considerations in llama2?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d88f0e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='ea779f07-b55a-47f6-aec3-3314d77910c8', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'g:\\\\Krish_Naik_LLMOps_Course\\\\Projects\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 76, 'page_label': '77'}, page_content='See evaluations for pretraining (Section 2); fine-tuning (Section 3); and safety (Section 4).\\nEthical Considerations and Limitations(Section 5.2)\\nLlama 2is a new technology that carries risks with use. Testing conducted to date has been in\\nEnglish, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs,\\nLlama 2’s potential outputs cannot be predicted in advance, and the model may in some instances'),\n",
       " Document(id='f2ee80c2-2ef2-44b4-98e4-a5b3e539a7ca', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'g:\\\\Krish_Naik_LLMOps_Course\\\\Projects\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 33, 'page_label': '34'}, page_content='5.2 Limitations and Ethical Considerations\\nLlama 2-Chatis subject to the same well-recognized limitations of other LLMs, including a cessation of\\nknowledge updates post-pretraining, potential for non-factual generation such as unqualified advice, and a\\npropensity towards hallucinations.\\nFurthermore, our initial version ofLlama 2-Chatpredominantly concentrated on English-language data.\\nWhile our experimental observations suggest the model has garnered some proficiency in other languages,'),\n",
       " Document(id='16bcd700-bb16-4f44-a100-a375ddcc81ad', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'g:\\\\Krish_Naik_LLMOps_Course\\\\Projects\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 35, 'page_label': '36'}, page_content='principlesofhelpfulnessandsafety. Tocontributemoresignificantlytosocietyandfosterthepaceofresearch,\\nwe have responsibly opened access toLlama 2andLlama 2-Chat. As part of our ongoing commitment to\\ntransparency and safety, we plan to make further improvements toLlama 2-Chatin future work.\\n36'),\n",
       " Document(id='621b0d8c-dd67-4eb4-9268-f71bd0d6f10d', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'g:\\\\Krish_Naik_LLMOps_Course\\\\Projects\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 33, 'page_label': '34'}, page_content='In addition, our study extended to evaluating theLlama 2-Chatwith access to a calculator. The results from\\nthis particular experiment are documented in Table 15. LLM tool use, while exciting, can also cause some\\nsafety concerns. We encourage more community research and red teaming in this area.\\n5.2 Limitations and Ethical Considerations\\nLlama 2-Chatis subject to the same well-recognized limitations of other LLMs, including a cessation of')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2335f39b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'See evaluations for pretraining (Section 2); fine-tuning (Section 3); and safety (Section 4).\\nEthical Considerations and Limitations(Section 5.2)\\nLlama 2is a new technology that carries risks with use. Testing conducted to date has been in\\nEnglish, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs,\\nLlama 2’s potential outputs cannot be predicted in advance, and the model may in some instances'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_doc[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a734ed5",
   "metadata": {},
   "source": [
    "'See evaluations for pretraining (Section 2); fine-tuning (Section 3); and safety (Section 4).\\nEthical Considerations and Limitations(Section 5.2)\\nLlama 2is a new technology that carries risks with use. Testing conducted to date has been in\\nEnglish, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs,\\nLlama 2’s potential outputs cannot be predicted in advance, and the model may in some instances'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43879815",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c7f0ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='ea779f07-b55a-47f6-aec3-3314d77910c8', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'g:\\\\Krish_Naik_LLMOps_Course\\\\Projects\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 76, 'page_label': '77'}, page_content='See evaluations for pretraining (Section 2); fine-tuning (Section 3); and safety (Section 4).\\nEthical Considerations and Limitations(Section 5.2)\\nLlama 2is a new technology that carries risks with use. Testing conducted to date has been in\\nEnglish, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs,\\nLlama 2’s potential outputs cannot be predicted in advance, and the model may in some instances'),\n",
       " Document(id='f2ee80c2-2ef2-44b4-98e4-a5b3e539a7ca', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'g:\\\\Krish_Naik_LLMOps_Course\\\\Projects\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 33, 'page_label': '34'}, page_content='5.2 Limitations and Ethical Considerations\\nLlama 2-Chatis subject to the same well-recognized limitations of other LLMs, including a cessation of\\nknowledge updates post-pretraining, potential for non-factual generation such as unqualified advice, and a\\npropensity towards hallucinations.\\nFurthermore, our initial version ofLlama 2-Chatpredominantly concentrated on English-language data.\\nWhile our experimental observations suggest the model has garnered some proficiency in other languages,'),\n",
       " Document(id='16bcd700-bb16-4f44-a100-a375ddcc81ad', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'g:\\\\Krish_Naik_LLMOps_Course\\\\Projects\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 35, 'page_label': '36'}, page_content='principlesofhelpfulnessandsafety. Tocontributemoresignificantlytosocietyandfosterthepaceofresearch,\\nwe have responsibly opened access toLlama 2andLlama 2-Chat. As part of our ongoing commitment to\\ntransparency and safety, we plan to make further improvements toLlama 2-Chatin future work.\\n36'),\n",
       " Document(id='621b0d8c-dd67-4eb4-9268-f71bd0d6f10d', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'g:\\\\Krish_Naik_LLMOps_Course\\\\Projects\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 33, 'page_label': '34'}, page_content='In addition, our study extended to evaluating theLlama 2-Chatwith access to a calculator. The results from\\nthis particular experiment are documented in Table 15. LLM tool use, while exciting, can also cause some\\nsafety concerns. We encourage more community research and red teaming in this area.\\n5.2 Limitations and Ethical Considerations\\nLlama 2-Chatis subject to the same well-recognized limitations of other LLMs, including a cessation of')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"what are limitations and ethical considerations in llama2?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e08ace2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Answer the question based on the context below. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd28dd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a1bc3f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2a6687c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"\\nAnswer the question based on the context below. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nContext: {context}\\n\\nQuestion: {question}\\n\\nAnswer: \")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "964d657a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a88eacc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e96acaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6492d988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b7b322e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\" : retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm                           \n",
    "    | parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1df59091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The table you provided shows the results of fine-tuning Llama 2 on various benchmarks. \\n\\nIt appears that the fine-tuning process was compared to ChatGPT, and the results are shown in the \"Fine-tuned\" section. The first row shows the results of fine-tuning Llama 2 with ChatGPT.\\n\\nHowever, to answer your question more comprehensively, the experiments also compared different sizes of the Llama 2 model (7B, 13B, 34B, and 70B) to an open-source base model, as shown in Table 3. The results suggest that larger model sizes generally lead to better performance on the grouped academic benchmarks.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"tell me about the llama2 finetuning benchmark experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aeeab526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From the given context, the SFT fine-tuning process involves:\\n\\n1. Collecting several thousand examples of high-quality SFT data, as illustrated in Table 5.\\n2. Using a cosine learning rate schedule with an initial setting (although the exact initial setting is not specified).\\n3. Stopping the annotation process after collecting a total of 27,540 annotations (although the initial number of annotations is not specified).\\n4. Applying a Rejection Sampling fine-tuning process initially, and later combining it with RLHF (Reward Learning with Human Feedback) sequentially.\\n5. Using a PPO (Proximal Policy Optimization) algorithm on top of the resulted Rejection Sampling checkpoint before sampling again.\\n\\nThe process also mentions that SFT annotations in the order of tens of thousands were enough to achieve a high-quality result, and that different annotation platforms and vendors can result in markedly different downstream performance.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"can you please explain the sft finetuning process?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd39150c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
