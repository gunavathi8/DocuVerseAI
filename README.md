# ğŸ“„ DocuVerse AI - an LLMOps Project ğŸš€

An end-to-end LLMOps-powered Document Intelligence Platform that enables users to interact with documents like never before. From semantic understanding to side-by-side comparison and intelligent querying â€” all deployed and automated using modern CI/CD practices on AWS.

'''

## âœ¨ Features

### ğŸ” Document Analysis

Automatically extract, summarize, and interpret content from uploaded documents using LLM + RAG pipeline.

### ğŸ†š Compare Two Documents

Upload any two documents and get a structured comparison.

### ğŸ’¬ Talk with a Single Document

Ask questions and get accurate answers from a single document using Retrieval-Augmented Generation (RAG).

### ğŸ“š Talk with Multiple Documents

Query across multiple related documents and get consolidated insights using intelligent chunking and vector search.

### âš™ï¸ CI/CD Pipeline

Fully automated Dev â†’ Test â†’ Deploy pipeline with version control, testing, linting, and push-to-AWS integration.

### â˜ï¸ AWS Deployment

Scalable and secure deployment using AWS services such as S3, EC2, Lambda, and API Gateway.

'''

## ğŸ§  Tech Stack
1. Layer	        Tech
2. Language	        Python ğŸ
3. LLM Framework	LangChain
4. RAG Pipeline	    Hugging Face Embeddings + FAISS/ChromaDB
5. LLMs Used	    OpenAI, Ollama, or any custom endpoint
6. Frontend	        Streamlit / FastAPI
7. CI/CD	        GitHub Actions + AWS CodePipeline
8. Deployment	    AWS EC2 / S3 / Lambda
9. Storage	        S3 (for documents), Vector DB for embeddings

'''

## Minimum Requirements

1. LLM Model - groq, openai, gemini, huggingface, claude, etc

2. Embedding Model - huggingface, openai, gemini

3. Vector DB - inmemory, persistent, cloude-based

'''

## ğŸ“‚ Project Structure

```plaintext
DOCUMENT_PORTAL/
â”‚
â”œâ”€â”€ config/                    # Configuration files
â”‚   â””â”€â”€ config.yaml            # Main app configuration
â”‚
â”œâ”€â”€ env/                       # Environment-related logic (if any)
â”‚
â”œâ”€â”€ exception/                 # Custom exception classes
â”‚   â””â”€â”€ custom_exception.py
â”‚
â”œâ”€â”€ logging/                   # Logging setup and formatters
â”‚
â”œâ”€â”€ notebook/                  # Jupyter experiments / test notebooks
â”‚   â””â”€â”€ experiments.ipynb
â”‚
â”œâ”€â”€ prompt_lib/                # Prompt templates and prompt builders
â”‚
â”œâ”€â”€ src/                       # Core application logic
â”‚   â”œâ”€â”€ doc_analyser/          # Modules for document content analysis
â”‚   â”œâ”€â”€ doc_compare/           # Logic for side-by-side document comparison
â”‚   â”œâ”€â”€ multidoc_chat/         # Querying across multiple documents
â”‚   â””â”€â”€ singledoc_chat/        # Interacting with a single document
â”‚
â”œâ”€â”€ static/                    # Static assets (if using Streamlit/FastAPI web UI)
â”‚
â”œâ”€â”€ templates/                 # HTML templates for web rendering
â”‚
â”œâ”€â”€ utils/                     # Helper utilities (S3 uploaders, embedding handlers, etc.)
â”‚
â”œâ”€â”€ .env                       # Local env vars (dev only â€“ API keys are managed via AWS Secrets Manager)
â”œâ”€â”€ app.py                     # Main app entrypoint (FastAPI or CLI)
â”œâ”€â”€ streamlit_ui.py            # Streamlit-based user interface
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ setup.py                   # Package/module setup file
â””â”€â”€ README.md                  # ğŸ“˜ Youâ€™re here!

```

## ğŸš€ How It Works
- Upload Documents â€” via UI or API.

- Text Extraction â€” from PDF, DOCX, PPTX, etc.

- Embedding + Storage â€” chunks are embedded and stored in Vector DB.

- RAG-Powered Responses â€” using LangChain + LLMs.

- Compare or Chat â€” depending on selected mode.

- CI/CD Pipeline â€” triggers auto-deploy to AWS on each commit.
